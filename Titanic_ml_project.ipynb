{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Yabudere/Yabu/blob/main/Titanic_ml_project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ny7AfSBw4Ss9",
        "outputId": "f37f19a7-6b16-49f1-d687-0f4ef3cc6f64"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HxXK4ph64T2r"
      },
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hl7W8pCf5Mgk"
      },
      "source": [
        "# **Load datasets**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8X17Di3U4T6h"
      },
      "outputs": [],
      "source": [
        "train_df = pd.read_csv('/content/drive/MyDrive/train (2).csv')\n",
        "test_df = pd.read_csv('/content/drive/MyDrive/test.csv')\n",
        "gender_submission_df = pd.read_csv('/content/drive/MyDrive/gender_submission (2).csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v4Febo5t6DeZ"
      },
      "source": [
        "# **Display the first few rows of the training dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lpveUGgL5Lsk"
      },
      "outputs": [],
      "source": [
        "train_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9mcSUkFf_9eG"
      },
      "source": [
        "**Check for missing datat**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eA8uVL7r4UCS"
      },
      "outputs": [],
      "source": [
        "train_df.info()\n",
        "train_df.isnull().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RQvfjZqF_7QS"
      },
      "source": [
        "### **Check  statistical summary to identify potential outliers**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_BoC001D4UFu"
      },
      "outputs": [],
      "source": [
        "train_df.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9mjLx1KgAu7X"
      },
      "source": [
        "**Visualize distributions to detect outliers**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VrAYFgDG4UJJ"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "sns.boxplot(data=train_df, x='Age')\n",
        "plt.show()\n",
        "\n",
        "sns.boxplot(data=train_df, x='Fare')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GLrNrBaKBesL"
      },
      "source": [
        "**Fill missing values in 'Age' with the median age**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cMIrFAZC4UQj"
      },
      "outputs": [],
      "source": [
        "train_df['Age'].fillna(train_df['Age'].median(), inplace=True)\n",
        "\n",
        "# Fill missing values in 'Embarked' with the mode\n",
        "train_df['Embarked'].fillna(train_df['Embarked'].mode()[0], inplace=True)\n",
        "\n",
        "# Drop the 'Cabin' column due to a large number of missing values\n",
        "train_df.drop(columns=['Cabin'], inplace=True)\n",
        "\n",
        "# Verify that there are no missing values left\n",
        "train_df.isnull().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WaTq85BiCBeH"
      },
      "source": [
        "**Convert categorical variables into dummy/indicator variables**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MhRORx9F4UT_"
      },
      "outputs": [],
      "source": [
        "train_df = pd.get_dummies(train_df, columns=['Sex', 'Embarked'], drop_first=True)\n",
        "\n",
        "# Drop irrelevant columns\n",
        "train_df.drop(columns=['Name', 'Ticket', 'PassengerId'], inplace=True)\n",
        "\n",
        "# Display the first few rows of the processed training dataset\n",
        "train_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dEbQ8TL44UXg"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_XVEiu7U4UbT"
      },
      "outputs": [],
      "source": [
        "# Define features (X) and target (y)\n",
        "X = train_df.drop(columns=['Survived'])\n",
        "y = train_df['Survived']\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d5UfIw8-DkXB"
      },
      "source": [
        "**Logistic Regression**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "54ooE9Hd4Uen"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Initialize and train Logistic Regression model\n",
        "logreg = LogisticRegression(max_iter=1000)\n",
        "logreg.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rLlDrAEdEEzR"
      },
      "source": [
        "**Decision Tree**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xGJ3NhSy4UiE"
      },
      "outputs": [],
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# Initialize and train Decision Tree model\n",
        "decision_tree = DecisionTreeClassifier(random_state=42)\n",
        "decision_tree.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7HwpTBd2EQiP"
      },
      "source": [
        "**Random Forest**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bWqcu0VW4UvV"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Initialize and train Random Forest model\n",
        "random_forest = RandomForestClassifier(random_state=42)\n",
        "random_forest.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2J7NhJd2Ewwq"
      },
      "source": [
        "**Model Evaluation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r1Q7LcOSarPs"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Function to evaluate model\n",
        "def evaluate_model(model, X_val, y_val):\n",
        "    y_pred = model.predict(X_val)\n",
        "    accuracy = accuracy_score(y_val, y_pred)\n",
        "    precision = precision_score(y_val, y_pred)\n",
        "    recall = recall_score(y_val, y_pred)\n",
        "    f1 = f1_score(y_val, y_pred)\n",
        "    return accuracy, precision, recall, f1\n",
        "\n",
        "# Evaluate Logistic Regression model\n",
        "logreg_metrics = evaluate_model(logreg, X_val, y_val)\n",
        "print(f\"Logistic Regression - Accuracy: {logreg_metrics[0]}, Precision: {logreg_metrics[1]}, Recall: {logreg_metrics[2]}, F1-score: {logreg_metrics[3]}\")\n",
        "\n",
        "# Evaluate Decision Tree model\n",
        "decision_tree_metrics = evaluate_model(decision_tree, X_val, y_val)\n",
        "print(f\"Decision Tree - Accuracy: {decision_tree_metrics[0]}, Precision: {decision_tree_metrics[1]}, Recall: {decision_tree_metrics[2]}, F1-score: {decision_tree_metrics[3]}\")\n",
        "\n",
        "# Evaluate Random Forest model\n",
        "random_forest_metrics = evaluate_model(random_forest, X_val, y_val)\n",
        "print(f\"Random Forest - Accuracy: {random_forest_metrics[0]}, Precision: {random_forest_metrics[1]}, Recall: {random_forest_metrics[2]}, F1-score: {random_forest_metrics[3]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pRVk49fhE9O2"
      },
      "source": [
        " **Model Tuning**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CPmPgvWyGgI8"
      },
      "outputs": [],
      "source": [
        "# Model Tuning\n",
        "\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Define parameter grid\n",
        "param_grid = {\n",
        "    'n_estimators': [100, 200, 300],\n",
        "    'max_depth': [None, 10, 20, 30],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4]\n",
        "}\n",
        "\n",
        "# Initialize GridSearchCV\n",
        "grid_search = GridSearchCV(estimator=random_forest, param_grid=param_grid, cv=3, n_jobs=-1, verbose=2)\n",
        "\n",
        "# Fit GridSearchCV\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Best parameters and model\n",
        "best_params = grid_search.best_params_\n",
        "best_model = grid_search.best_estimator_\n",
        "\n",
        "# Evaluate best model\n",
        "best_model_metrics = evaluate_model(best_model, X_val, y_val)\n",
        "print(f\"Best Random Forest - Accuracy: {best_model_metrics[0]}, Precision: {best_model_metrics[1]}, Recall: {best_model_metrics[2]}, F1-score: {best_model_metrics[3]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " **Evaluating and Tuning Random Forest Model Performance Using GridSearchCV**"
      ],
      "metadata": {
        "id": "WKGTLI7smml5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kvbGtBU4GgXv"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Define parameter grid\n",
        "param_grid = {\n",
        "    'n_estimators': [100, 200, 300],\n",
        "    'max_depth': [None, 10, 20, 30],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4]\n",
        "}\n",
        "\n",
        "# Initialize GridSearchCV\n",
        "grid_search = GridSearchCV(estimator=random_forest, param_grid=param_grid, cv=3, n_jobs=-1, verbose=2)\n",
        "\n",
        "# Fit GridSearchCV\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Best parameters and model\n",
        "best_params = grid_search.best_params_\n",
        "best_model = grid_search.best_estimator_\n",
        "\n",
        "# Evaluate best model\n",
        "best_model_metrics = evaluate_model(best_model, X_val, y_val)\n",
        "print(f\"Best Random Forest - Accuracy: {best_model_metrics[0]}, Precision: {best_model_metrics[1]}, Recall: {best_model_metrics[2]}, F1-score: {best_model_metrics[3]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Evaluation Metrics for the Best Model**"
      ],
      "metadata": {
        "id": "zruHqjztnFtf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# To Set Seaborn style\n",
        "sns.set(style=\"whitegrid\")\n",
        "\n",
        "# Hyperparameter tuning for Logistic Regression\n",
        "param_grid = {'C': [0.1, 1, 10, 100]}\n",
        "grid_search = GridSearchCV(LogisticRegression(max_iter=1000), param_grid, cv=3)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Best parameters and model\n",
        "best_model = grid_search.best_estimator_\n",
        "\n",
        "# To Evaluate the best model\n",
        "y_pred = best_model.predict(X_val)\n",
        "accuracy = accuracy_score(y_val, y_pred)\n",
        "precision = precision_score(y_val, y_pred)\n",
        "recall = recall_score(y_val, y_pred)\n",
        "f1 = f1_score(y_val, y_pred)\n",
        "\n",
        "# To Plot the evaluation metrics\n",
        "metrics = ['Accuracy', 'Precision', 'Recall', 'F1-score']\n",
        "values = [accuracy, precision, recall, f1]\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "ax = sns.barplot(x=metrics, y=values, palette='viridis')\n",
        "\n",
        "# To Add value labels on the bars\n",
        "for i, value in enumerate(values):\n",
        "    ax.text(i, value + 0.02, f\"{value:.2f}\", ha='center', va='bottom', fontsize=12)\n",
        "\n",
        "# To Customize plot\n",
        "plt.xlabel('Metrics', fontsize=14)\n",
        "plt.ylabel('Score', fontsize=14)\n",
        "plt.title('Evaluation Metrics for the Best Model', fontsize=16)\n",
        "plt.ylim(0, 1)  # Assuming the scores are between 0 and 1\n",
        "\n",
        "# To Show the plot\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "8eMLWopVjWTU"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOYoNmAFE7S6IjFBxwNluLY",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}